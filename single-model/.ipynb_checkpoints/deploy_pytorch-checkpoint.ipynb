{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Hugging Face BART transformer model in Amazon SageMaker \n",
    "\n",
    "This notebook is a step-by-step tutorial on deploying a pre-trained Hugging Face model [BART](https://huggingface.co/transformers/model_doc/bart.html) on [PyTorch](https://pytorch.org/) framework. Bart uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT). Specifically, we use the BART Model with a language modeling head [BartForConditionalGeneration](https://huggingface.co/transformers/model_doc/bart.html#transformers.BartForConditionalGeneration) for summarization task. \n",
    "\n",
    "We will describe the steps for deploying this model similar to any other PyTorch model on Amazon SageMaker with TorchServe serving stack. For training Hugging Face models on SageMaker, refer the examples [here](https://github.com/huggingface/notebooks/tree/master/sagemaker)\n",
    "\n",
    "The outline of steps is as follows:\n",
    "\n",
    "1. Download pre-trained Hugging Face model\n",
    "2. Save and upload model artifact to S3\n",
    "2. Create an inference entrypoint script\n",
    "3. Deploy endpoint\n",
    "4. Trigger endpoint invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import boto3\n",
    "import torch\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='<enter-bucket-name>'\n",
    "prefix='bart-model-pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-us-east-1-208480242416'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Hugging Face pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==4.2.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download a pre-tuned bart transformer and move the model artifact to  S3 bucket\n",
    "PRE_TRAINED_MODEL_NAME='facebook/bart-large-cnn'\n",
    "model = BartForConditionalGeneration.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and upload model archive to S3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pth\r\n"
     ]
    }
   ],
   "source": [
    "!tar -cvzf model.tar.gz model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-208480242416/bart-model-pytorch/data/raw/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "file_key = 'model.tar.gz'\n",
    "model_artifact = S3Uploader.upload(file_key,'s3://{}/{}/data/raw'.format(bucket, prefix))\n",
    "print(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an inference entrypoint script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\r\n",
      "# SPDX-License-Identifier: MIT-0\r\n",
      "\r\n",
      "import logging  \r\n",
      "import time as time\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import config \r\n",
      "import torch \r\n",
      "\r\n",
      "\r\n",
      "from typing import List, Dict\r\n",
      "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\r\n",
      "\r\n",
      "logger = logging.getLogger(__name__)\r\n",
      "\r\n",
      "PRE_TRAINED_MODEL_NAME = 'facebook/bart-large-cnn'\r\n",
      "tokenizer = BartTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\r\n",
      "def model_fn(model_dir):\r\n",
      "    # load bart model\r\n",
      "    start_load = time.time()\r\n",
      "    model_path = f'{model_dir}/model.pth'\r\n",
      "    logger.info(model_path)\r\n",
      "    device = get_device()\r\n",
      "    logger.info('device is ')\r\n",
      "    logger.info(device)\r\n",
      "    model = BartForConditionalGeneration(BartConfig())\r\n",
      "    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\r\n",
      "    model.to(device)\r\n",
      "    logger.info(f\"BART model () loaded in {time.time() - start_load} s\")\r\n",
      "    return model\r\n",
      "\r\n",
      "def get_device():\r\n",
      "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\r\n",
      "    return device\r\n",
      "\r\n",
      "def predict_fn(batches, model):\r\n",
      "    logger.info('starting inference')\r\n",
      "    #logger.info(batches)\r\n",
      "    logger.info(type(batches))   \r\n",
      "    summaries = []\r\n",
      "    start = time.time()\r\n",
      "    logger.info(\"Starting inference for summarization...\")\r\n",
      "    for batch in batches:\r\n",
      "        encoded_batch = tokenizer.batch_encode_plus(batch[\"samples\"], max_length=1024, truncation=True,\r\n",
      "                                                    return_tensors='pt', padding=True, add_prefix_space=True)\r\n",
      "        \r\n",
      "        encoded_batch = encoded_batch.to(device=device)\r\n",
      "        ids = model.generate(**encoded_batch, max_length=batch[\"max_len\"], min_length=batch[\"min_len\"], num_beams=3)\r\n",
      "        inference = tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\r\n",
      "        summaries += inference\r\n",
      "        logger.info(f\"Batch with {len(batch['samples'])} samples \"\r\n",
      "                    f\"(max: {batch['max_len']}, min: {batch['min_len']}), took {time.time() - start} s\")\r\n",
      "\r\n",
      "    logger.info(\"Done summarizing paragraphs!\")\r\n",
      "    #summaries = postprocessing(summaries, sorting_order)\r\n",
      "\r\n",
      "    return summaries\r\n",
      "\r\n",
      "def input_fn(serialized_input_data, content_type='application/json'):  \r\n",
      "    logger.info('starting input fn')\r\n",
      "    input_data = json.loads(serialized_input_data)\r\n",
      "    paragraphs = [\" \".join(text.split()) for text in input_data]\r\n",
      "    batches, sorting_order = create_batches(paragraphs, tokenizer, float('0.2'))\r\n",
      "    return batches\r\n",
      "    \r\n",
      "def output_fn(prediction_output, accept='application/json'):\r\n",
      "    logger.info('starting output fn')\r\n",
      "    return json.dumps(prediction_output), accept\r\n",
      "    \r\n",
      "def format_batch(samples, min_tokens, delta):\r\n",
      "    \"\"\"\r\n",
      "    Format a batch for inference, computing the min_len and max_len\r\n",
      "    Args:\r\n",
      "        samples (List[str]): list of samples to put in the batch\r\n",
      "        min_tokens (int): the minimum number of tokens of the longest sample\r\n",
      "        delta (int): number of tokens that correspond to threshold tolerance for the batch\r\n",
      "\r\n",
      "    Returns:\r\n",
      "        (dict): formatted batch\r\n",
      "    \"\"\"\r\n",
      "    # if single batch don't use delta on min tokens\r\n",
      "    min_len = min_tokens if len(samples) == 1 else min_tokens - delta\r\n",
      "    max_len = min_tokens + config.MAX_DELTA_TOKENS\r\n",
      "\r\n",
      "    return {\"samples\": samples, \"max_len\": int(max_len), \"min_len\": int(min_len)}\r\n",
      "\r\n",
      "def create_batches(paragraphs, tokenizer, reduction_percentage):\r\n",
      "\r\n",
      "    # compute lengths of each sample and reorder in descending length\r\n",
      "    lengths = [min(1024, len(tokenizer.encode(p))) for p in paragraphs]\r\n",
      "    sorted_indexes = np.argsort(lengths)[::-1].tolist()\r\n",
      "\r\n",
      "    inference_batches, current_batch = [], []\r\n",
      "    min_tokens = lengths[sorted_indexes[0]] * reduction_percentage\r\n",
      "    delta = min_tokens * config.THRESHOLD\r\n",
      "\r\n",
      "    for idx in sorted_indexes:\r\n",
      "\r\n",
      "        # handle very short paragraphs\r\n",
      "        if lengths[idx] <= config.MAX_DELTA_TOKENS:\r\n",
      "            # save previous batch, if any\r\n",
      "            if current_batch:\r\n",
      "                inference_batches.append(format_batch(current_batch, min_tokens, delta))\r\n",
      "\r\n",
      "            span_idx = sorted_indexes.index(idx)\r\n",
      "            current_batch = [paragraphs[i] for i in sorted_indexes[span_idx:]]\r\n",
      "            min_tokens = lengths[idx]\r\n",
      "            delta = min_tokens - lengths[sorted_indexes[-1]]  # min_len will be set to len of shortest paragraph\r\n",
      "            break\r\n",
      "\r\n",
      "        tokens = int(lengths[idx] * reduction_percentage)\r\n",
      "        if tokens < min_tokens - delta or len(current_batch) >= config.MAX_BSZ:\r\n",
      "            # save previous batch, if any\r\n",
      "            if current_batch:\r\n",
      "                inference_batches.append(format_batch(current_batch, min_tokens, delta))\r\n",
      "            current_batch = [paragraphs[idx]]\r\n",
      "            min_tokens = tokens\r\n",
      "            delta = int(tokens * config.THRESHOLD)\r\n",
      "        else:\r\n",
      "            current_batch.append(paragraphs[idx])\r\n",
      "\r\n",
      "    # save last batch\r\n",
      "    inference_batches.append(format_batch(current_batch, min_tokens, delta))\r\n",
      "\r\n",
      "    return inference_batches, sorted_indexes\r\n"
     ]
    }
   ],
   "source": [
    "!cat source_dir/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy PyTorchModel to a SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super().__init__(endpoint_name, sagemaker_session=sagemaker_session, serializer=JSONSerializer(), \n",
    "                         deserializer=JSONDeserializer())\n",
    "model = PyTorchModel(model_data=model_artifact,\n",
    "                   name=name_from_base('summarizer'),\n",
    "                   role=role, \n",
    "                   entry_point='inference.py',\n",
    "                   source_dir='source_dir',\n",
    "                   py_version=\"py3\",\n",
    "                   framework_version='1.6.0',\n",
    "                   predictor_cls=Summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = name_from_base('summarizer') \n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.g4dn.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = {\"paragraphs\": [\"A Guide to Managing Your (Newly) Remote Workers\\nWith the Covid-19 epidemic, many employees and their managers are finding themselves working out of the office and separated from each other for the first time.\\n Fortunately, there are specific, research-based steps that managers can take without great effort to improve the engagement and productivity of remote employees, even when there is little time to prepare.\\n First, it s important to understand the common challenges, from isolation to distractions to lack of face-to-face supervision.\\n Then managers can support remote workers with 1) regular, structured check-ins; 2) multiple communication options (and established norms for each; 3) opportunities for social interactions; and 4) ongoing encouragement and emotional support.\\nLeer en espa\\u00f1ol In these difficult times, we ve made a number of our coronavirus articles free for all readers.\\n To get all of HBR s content delivered to your inbox, sign up for the Daily Alert newsletter.\\nIn response to the uncertainties presented by Covid-19, many companies and universities have asked their employees to work remotely.\\n While close to a quarter of the U.\\nS.\\n workforce already works from home at least part of the time, the new policies leave many employees and their managers working out of the office and separated from each other for the first time.\\nAlthough it is always preferable to establish clear remote-work policies and training in advance, in times of crisis or other rapidly changing circumstances, this level of preparation may not be feasible.\\n Fortunately, there are specific, research-based steps that managers can take without great effort to improve the engagement and productivity of remote employees, even when there is little time to prepare.\\nCommon Challenges of Remote Work\\n\", \"To start, managers need to understand factors that can make remote work especially demanding.\\n Otherwise high-performing employees may experience declines in job performance and engagement when they begin working remotely, especially in the absence of preparation and training.\\n Challenges inherent in remote work include:\\nLack of face-to-face supervision: Both managers and their employees often express concerns about the lack of face-to-face interaction.\\n Supervisors worry that employees will not work as hard or as efficiently (though research indicates otherwise, at least for some types of jobs).\\n Many employees, on the other hand, struggle with reduced access to managerial support and communication.\\n In some cases, employees feel that remote managers are out of touch with their needs, and thereby are neither supportive nor helpful in getting their work done.\\nLack of access to information: Newly remote workers are often surprised by the added time and effort needed to locate information from coworkers.\\n Even getting answers to what seem like simple questions can feel like a large obstacle to a worker based at home.\\nThis phenomenon extends beyond task-related work to interpersonal challenges that can emerge among remote coworkers.\\n Research has found that a lack of mutual knowledge among remote workers translates to a lower willingness to give coworkers the benefit of the doubt in difficult situations.\\n For example, if you know that your officemate is having a rough day, you will view a brusque email from them as a natural product of their stress.\\n However, if you receive this email from a remote coworker, with no understanding of their current circumstances, you are more likely to take offense, or at a minimum to think poorly of your coworker s professionalism.\\n\", \"Social isolation: Loneliness is one of the most common complaints about remote work, with employees missing the informal social interaction of an office setting.\\n It is thought that extraverts may suffer from isolation more in the short run, particularly if they do not have opportunities to connect with others in their remote-work environment.\\n However, over a longer period of time, isolation can cause any employee to feel less belonging to their organization, and can even result in increased intention to leave the company.\\nDistractions at home: We often see photos representing remote work which portray a parent holding a child and typing on a laptop, often sitting on a sofa or living-room floor.\\n In fact, this is a terrible representation of effective virtual work.\\n Typically, we encourage employers to ensure that their remote workers have both dedicated workspace and adequate childcare before allowing them to work remotely.\\n Yet, in the case of a sudden transition to virtual work, there is a much greater chance that employees will be contending with suboptimal workspaces and (in the case of school and daycare closures) unexpected parenting responsibilities.\\n Even in normal circumstances family and home demands can impinge on remote work; managers should expect these distractions to be greater during this unplanned work-from-home transition.\\nHow Managers Can Support Remote Employees\\nAs much as remote work can be fraught with challenges, there are also relatively quick and inexpensive things that managers can do to ease the transition.\\n Actions that you can take today include:\\n\", \"Establish structured daily check-ins: Many successful remote managers establish a daily call with their remote employees.\\n This could take the form of a series of one-on-one calls, if your employees work more independently from each other, or a team call, if their work is highly collaborative.\\n The important feature is that the calls are regular and predictable, and that they are a forum in which employees know that they can consult with you, and that their concerns and questions will be heard.\\n\", \"Provide several different communication technology options: Email alone is insufficient.\\n Remote workers benefit from having a richer technology, such as video conferencing, that gives participants many of the visual cues that they would have if they were face-to-face.\\n Video conferencing has many advantages, especially for smaller groups: Visual cues allow for increased mutual knowledge about coworkers and also help reduce the sense of isolation among teams.\\n Video is also particularly useful for complex or sensitive conversations, as it feels more personal than written or audio-only communication.\\nThere are other circumstances when quick collaboration is more important than visual detail.\\n For these situations, provide mobile-enabled individual messaging functionality (like Slack, Zoom, Microsoft Teams, etc.\\n) which can be used for simpler, less formal conversations, as well as time-sensitive communication.\\nIf your company doesn t have technology tools already in place, there are inexpensive ways to obtain simple versions of these tools for your team, as a short-term fix.\\n Consult with your organization s IT department to ensure there is an appropriate level of data security before using any of these tools.\\nAnd then establish rules of engagement : Remote work becomes more efficient and satisfying when managers set expectations for the frequency, means, and ideal timing of communication for their teams.\\n For example, We use videoconferencing for daily check-in meetings, but we use IM when something is urgent.\\n Also, if you can, let your employees know the best way and time to reach you during the workday (e.\\ng.\\n, I tend to be more available late in the day for ad hoc phone or video conversations, but if there s an emergency earlier in the day, send me a text.\\n ) Finally, keep an eye on communication among team members (to the extent appropriate), to ensure that they are sharing information as needed.\\nWe recommend that managers establish these rules of engagement with employees as soon as possible, ideally during the first online check-in meeting.\\n While some choices about specific expectations may be better than others, the most important factor is that all employees share the same set of expectations for communication.\\nProvide opportunities for remote social interaction: One of the most essential steps a manager can take is to structure ways for employees to interact socially (that is, have informal conversations about non-work topics) while working remotely.\\n This is true for all remote workers, but particularly so for workers who have been abruptly transitioned out of the office.\\nThe easiest way to establish some basic social interaction is to leave some time at the beginning of team calls just for non-work items (e.\\ng.\\n, We re going to spend the first few minutes just catching up with each other.\\n How was your weekend? ).\\n Other options include virtual pizza parties (in which pizza is delivered to all team members at the time of a videoconference), or virtual office parties (in which party care packages can be sent in advance to be opened and enjoyed simultaneously).\\n While these types of events may sound artificial or forced, experienced managers of remote workers (and the workers themselves) report that virtual events help reduce feelings of isolation, promoting a sense of belonging.\\nOffer encouragement and emotional support: Especially in the context of an abrupt shift to remote work, it is important for managers to acknowledge stress, listen to employees anxieties and concerns, and empathize with their struggles.\\n If a newly remote employee is clearly struggling but not communicating stress or anxiety, ask them how they re doing.\\n Even a general question such as How is this remote work situation working out for you so far? can elicit important information that you might not otherwise hear.\\n Once you ask the question, be sure to listen carefully to the response, and briefly restate it back to the employee, to ensure that you understood correctly.\\n Let the employee s stress or concerns (rather than your own) be the focus of this conversation.\\nResearch on emotional intelligence and emotional contagion tells us that employees look to their managers for cues about how to react to sudden changes or crisis situations.\\n If a manager communicates stress and helplessness, this will have what Daniel Goleman calls a trickle-down effect on employees.\\n Effective leaders take a two-pronged approach, both acknowledging the stress and anxiety that employees may be feeling in difficult circumstances, but also providing affirmation of their confidence in their teams, using phrases such as we ve got this, or this is tough, but I know we can handle it, or let s look for ways to use our strengths during this time.\\n With this support, employees are more likely to take up the challenge with a sense of purpose and focus.\\nWe ll add our own note of encouragement to managers facing remote work for the first time: you ve got this.\\n Let us know in the comments your own tips for managing your remote employees.\\n\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cfb30fdd2600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraphs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "prediction = predictor.predict((raw_text['paragraphs']))\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6564e5e324aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
